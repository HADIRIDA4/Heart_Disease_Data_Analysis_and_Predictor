{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_data_handler ,database_handler,lookups\n",
    "import prehook\n",
    "import lookups \n",
    "from misc_handler import return_lookup_items_as_dict\n",
    "import hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_list,df_src_titles=prehook.execute_prehook(sql_command_directory_path = 'SQL_COMMANDS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\Data-Analysis-and-Visualization-of-Heart-Diseases\\database_handler.py:70: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return_dataframe = pd.read_sql_query(con= db_session, sql= file_executor)\n"
     ]
    }
   ],
   "source": [
    "hook.execute_hook(df_src_titles,df_src_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from database_handler import create_connection, execute_query, return_data_as_df\n",
    "from hook import return_etl_last_updated_date\n",
    "from logging_handler import show_error_message\n",
    "from lookups import DestinationDatabase, ETLStep, ErrorHandling, InputTypes\n",
    "from pandas_data_handler import return_create_statement_from_df, return_insert_into_sql_statement_from_df\n",
    "\n",
    "\n",
    "def create_insert_sql(db_session, source_name,df_source_list,df_titles,etl_step,input_type=None,etl_date = None):\n",
    "    \n",
    "    try:\n",
    "        source_name = source_name.value\n",
    "        for df_source, df_title in zip(df_titles,df_source_list):\n",
    "            dst_table = f\"stg_{source_name}_{df_title}\"\n",
    "            if etl_step == ETLStep.PRE_HOOK:\n",
    "                dataframe_source=return_data_as_df(df_source, input_type)\n",
    "                create_stmt = return_create_statement_from_df(dataframe_source, 'dw_reporting', dst_table)\n",
    "                execute_query(db_session=db_session, query= create_stmt)\n",
    "\n",
    "                # index_name = df_source.index.name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "                # create_sql_staging_table_index(db_session, 'dw_reporting', dst_table, index_name)\n",
    "            elif etl_step == ETLStep.HOOK:\n",
    "                dataframe_source=return_data_as_df(df_title, input_type)\n",
    "                latest_date=pd.to_datetime(dataframe_source['last_update']).max()\n",
    "                etl_date = pd.to_datetime(etl_date)\n",
    "                if latest_date>etl_date:\n",
    "                    staging_df=dataframe_source\n",
    "                if len(staging_df):\n",
    "                    insert_stmt = return_insert_into_sql_statement_from_df(dataframe_source, 'dw_reporting', dst_table)\n",
    "                    # execute_query(db_session=db_session, query= insert_stmt)\n",
    "                    return insert_stmt\n",
    "    except Exception as error:\n",
    "        suffix = str(error)\n",
    "        error_prefix = ErrorHandling.CREATE_INSERT_STAGING_TABLES_ERROR\n",
    "        show_error_message(error_prefix.value, suffix)\n",
    "        raise Exception(\"Error creating/insert into staging tables\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "db_session = create_connection()\n",
    "etl_date, does_etl_time_exists = return_etl_last_updated_date(db_session)\n",
    "z=create_insert_sql(db_session,DestinationDatabase.DATABASE_NAME,df_src_titles,df_src_list,ETLStep.HOOK,InputTypes.CSV,etl_date)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from database_handler import execute_query, return_data_as_df\n",
    "from logging_handler import show_error_message\n",
    "from lookups import DestinationDatabase, ETLStep, ErrorHandling, studies\n",
    "from misc_handler import enum_to_lists\n",
    "from pandas_data_handler import return_create_statement_from_df, return_insert_into_sql_statement_from_df\n",
    "import pandas as pd\n",
    "import database_handler\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_insert_sql(db_session, source_name,df_source_list,df_titles,etl_step,input_type=None,etl_date = None):\n",
    "    \n",
    "    try:\n",
    "        source_name = source_name.value\n",
    "        for df_source, df_title in zip(df_titles,df_source_list):\n",
    "            dst_table = f\"stg_{source_name}_{df_source}\"\n",
    "            if etl_step == ETLStep.PRE_HOOK:\n",
    "                dataframe_source=return_data_as_df(df_source, input_type)\n",
    "                create_stmt = return_create_statement_from_df(dataframe_source, 'dw_reporting', dst_table)\n",
    "                execute_query(db_session=db_session, query= create_stmt)\n",
    "\n",
    "                # index_name = df_source.index.name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "                # create_sql_staging_table_index(db_session, 'dw_reporting', dst_table, index_name)\n",
    "            elif etl_step == ETLStep.HOOK:\n",
    "                dataframe_source=return_data_as_df(df_title, input_type)\n",
    "                # latest_date=pd.to_datetime(dataframe_source['last_update']).max()\n",
    "                # etl_date_2=datetime.strptime(etl_date, \"%Y-%m-%d\")\n",
    "                # if latest_date<etl_date_2\n",
    "                staging_df=dataframe_source\n",
    "                if len(staging_df):\n",
    "                    insert_stmt = return_insert_into_sql_statement_from_df(dataframe_source, 'dw_reporting', dst_table)\n",
    "                    execute_query(db_session=db_session, query= insert_stmt)\n",
    "    except Exception as error:\n",
    "        suffix = str(error)\n",
    "        error_prefix = ErrorHandling.CREATE_INSERT_STAGING_TABLES_ERROR\n",
    "        show_error_message(error_prefix.value, suffix)\n",
    "        raise Exception(\"Error creating/insert into staging tables\")\n",
    "etl_date = \"2001-04-02\"\n",
    "\n",
    "df_title,source=enum_to_lists(studies)\n",
    "db_session=database_handler.create_connection()\n",
    "create_insert_sql(db_session,DestinationDatabase.DATABASE_NAME,source, df_title, ETLStep.HOOK, lookups.InputTypes.CSV,etl_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lookups import ETLStep,DestinationDatabase,ErrorHandling\n",
    "from database_handler import execute_query\n",
    "\n",
    "\n",
    "def execute_sql_folder(db_session, sql_command_directory_path, etl_step, target_schema):\n",
    "    sql_files = [sqlfile for sqlfile in os.listdir(sql_command_directory_path) if sqlfile.endswith('.sql')]\n",
    "    sorted_sql_files = sorted(sql_files, key=lambda x: int(x[1:x.index('-')]))\n",
    "    counter = 0\n",
    "    for sql_file in sorted_sql_files:\n",
    "        counter+=1\n",
    "        file_title = sql_file.split('-')\n",
    "        if file_title[1] == etl_step.value:\n",
    "            with open(os.path.join(sql_command_directory_path,sql_file), 'r') as file:\n",
    "                sql_query = file.read()\n",
    "                sql_query = sql_query.replace('target_schema', target_schema.value)\n",
    "                return_val = execute_query(db_session= db_session, query= sql_query)\n",
    "                if not return_val == ErrorHandling.NO_ERROR:\n",
    "                    raise Exception(f\"Error executing SQL File on = \" +  str(sql_file))\n",
    "                \n",
    "db_session=database_handler.create_connection()\n",
    "execute_sql_folder(db_session,\"SQL_COMMANDS\",ETLStep.HOOK,DestinationDatabase.SCHEMA_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
